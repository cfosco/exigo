<!DOCTYPE html>
<html lang="en">

    <head>

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <meta name="description" content="">
        <meta name="author" content="">

        <title>Exigo - Interpretability made easy</title>

        <!-- Bootstrap core CSS -->
        <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

        <!-- Custom fonts for this template -->
        <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
        <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
        <link href='https://fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic,900,900italic' rel='stylesheet' type='text/css'>

        <!-- Plugin CSS -->
        <link href="vendor/magnific-popup/magnific-popup.css" rel="stylesheet">

        <!-- Custom styles for this template -->
        <link href="css/creative.css" rel="stylesheet">
        <link rel="apple-touch-icon" sizes="57x57" href="img/favicon/apple-icon-57x57.png">
        <link rel="apple-touch-icon" sizes="60x60" href="img/favicon/apple-icon-60x60.png">
        <link rel="apple-touch-icon" sizes="72x72" href="img/favicon/apple-icon-72x72.png">
        <link rel="apple-touch-icon" sizes="76x76" href="img/favicon/apple-icon-76x76.png">
        <link rel="apple-touch-icon" sizes="114x114" href="img/favicon/apple-icon-114x114.png">
        <link rel="apple-touch-icon" sizes="120x120" href="img/favicon/apple-icon-120x120.png">
        <link rel="apple-touch-icon" sizes="144x144" href="img/favicon/apple-icon-144x144.png">
        <link rel="apple-touch-icon" sizes="152x152" href="img/favicon/apple-icon-152x152.png">
        <link rel="apple-touch-icon" sizes="180x180" href="img/favicon/apple-icon-180x180.png">
        <link rel="icon" type="image/png" sizes="192x192"  href="/img/favicon/android-icon-192x192.png">
        <link rel="icon" type="image/png" sizes="32x32" href="img/favicon/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="96x96" href="img/favicon/favicon-96x96.png">
        <link rel="icon" type="image/png" sizes="16x16" href="img/favicon/favicon-16x16.png">
        <link rel="manifest" href="img/favicon/manifest.json">
        <meta name="msapplication-TileColor" content="#ffffff">
        <meta name="msapplication-TileImage" content="img/favicon/ms-icon-144x144.png">
        <meta name="theme-color" content="#ffffff">


    </head>

    <body id="page-top">

        <!-- Navigation -->
        <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
            <div class="container">
                <a class="navbar-brand js-scroll-trigger" href="#page-top">Exigo</a>
                <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ml-auto">
                        <li class="nav-item">
                            <a class="nav-link js-scroll-trigger" href="#about">Overview</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link js-scroll-trigger" href="#eda">EDA</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link js-scroll-trigger" href="#lit">Lit Review</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link js-scroll-trigger" href="#approach">Approach</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link js-scroll-trigger" href="#demo">Try Exigo</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link js-scroll-trigger" href="#conclusion">Conclusion</a>
                        </li>
                    </ul>
                </div>
            </div>
        </nav>



        <header>
            <div id="sketch-holder"> </div>
            <div class="header-content">
                <div class="header-content-inner">
                    <h1 id="main_title">Interpretability made easy</h1>
                    <hr>
                    <p>Exigo helps you understand your models. Using the latest interpretation and fairness methods,
                    Exigo generates a complete report that describes your model and data while giving insights on how to improve your systems.</p>
                    <a class="btn btn-primary btn-xl js-scroll-trigger" href="#demo">Try it out</a>
                </div>
            </div>
        </header>


        <section class="bg-primary" id="about">
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 mx-auto text-center">
                        <h2 class="section-heading text-white">Exigo - Overview</h2>
                        <hr class="light my-4">
                        <p class="text-faded mb-4">Exigó (εξηγώ) means "to explain" in Greek. We offer a dashboard for machine learning models
                        that does just that: it explains, in various ways, the clockwork of lending models.
                        Our dashboard will give credit analysts insights into the accuracy,
                        fairness and reasons behind the model's decisions.  We hope the dashboard will increase both a
                        credit analyst's speed to make a decision and accuracy in making the right choices.</p>

                        <h2 class="section-heading text-white">What is the goal of the project?</h2>
                        <p class="text-faded mb-4">The high-level goal of our project was to create a tool that will be practically
                        useful for credit analysts making decisions about which applicants to give loans to
                        and which applicants to deny.  We approach this problem with a dashboard that displays
                        pertinent information for a credit analyst to make a decision with the aid of a
                        machine learning model.  Credit analysts will not only be aided by the model, but they
                        will be able to see <em>why</em> the model made the decision it made.</p>

                        <h2 class="section-heading text-white">Why does anyone care?</h2>

                        <p class="text-faded mb-4">This project is especially important in our current day and age. We are in a era where
                        machine learning algorithms are rapidly growing in popularity
                        and prevalence.  These algorithms are making more and more decisions that effect people
                        on a daily basis.  Since the algorithms are only as good as the data we feed into them, they
                        are susceptible to taking in and actually <em>replicating</em> a bias toward or against a
                        protected class.  This is a deeply rooted issue because the bias might not just be in a
                        "sensitive" feature like race or religion that is in the dataset, but can be carried through
                        other, allegedly safe, features that can act as proxies for the protected class. There is a ethical
                        motivation to test our decision-making algorithms to ensure they are not biased against certain
                        protected classes. In addition to treating all people <em>fairly</em>, in the context of lending, we
                        need to be able to give reasons why a potential customer was denied a loan. Giving reasons for why
                        a customer was denied a loan is not only useful for the customer, but required by
                        General Data Protection Regulation (GDPR).</p>
                        <a class="btn btn-light btn-xl js-scroll-trigger mt-3" href="#demo">Show me the demo</a>
                    </div>
                </div>
            </div>
        </section>


<!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<!-- +++++++++++++++++++++++++ EDA SECTION +++++++++++++++++++++++++++++ -->
<!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->


        <section id="EDA">
            <script src="https://code.jquery.com/jquery-1.10.1.min.js"></script>
            <script src="https://d3js.org/d3.v3.min.js" charset="utf-8"></script>
            <script src="js/helper.js" type='text/javascript'></script>
            <div class="container" id="eda">
                <div class="row">
                    <div class="col-lg-12 text-center">
                        <h2 class="section-heading">Exploratory Data Analysis</h2>
                        <hr class="my-4">
                        <p>Before we can really attack a problem, we have to know the data we are using inside and out.
                        We are using a publicly available dataset from LendingClub that dates from 2007-2017 and
                        has around 1.6 million loans and 151 different features.  Although the dataset has 1.6
                        million total loans, Just under 50% of them are current loans that have not either
                        been defaulted on or paid back.  We will ignore these loans for modeling purposes,
                        which leaves us with a dataset of around 850k loans. </p>
                        <style>
.node rect {
    cursor: move;
    fill-opacity: .9;
    shape-rendering: crispEdges;
}

            .node text {
                pointer-events: none;
                text-shadow: 0 1px 0 #fff;
            }

            .link {
                fill: none;
                /*stroke: #000;*/
                stroke-opacity: .5;
            }

            .link:hover {
                stroke-opacity: .8;
            }
                        </style>

                        <p id="chart1">

                        <script>  sankey_plot("data/grade_data.json") </script>

                        <div class="container-fluid" href="#eda">
                            <div class="text-center">
                                <div class="service-box mx-auto" >
                                    <img src="img/wc_viz.png" class='img-fluid'></img>
                                    <h3 class="mb-3">What words come up with loan requests?</h3>
                                    <p class="text-muted mb-0">The words in this word cloud are the terms that come up most often as the reason for a potential client requesting a loan.</p>
                                </div>
                            </div>
                        </div>

                        <div class="container-fluid">
                            <div class="text-center">
                                <div class="service-box mx-auto">
                                    <img src="img/pie_chart.png" class='img-fluid'></img>
                                    <h3 class="mb-3">What states do most of the loan applications come from?</h3>
                                    <p class="text-muted mb-0">As we would expect, the two most prevalent states are California and New York.</p>
                                </div>
                            </div>
                        </div>

                        <div class="row">
                            <div class="col-lg-3 text-center" style="margin: auto">
                                <div class="service-box mt-5 mx-auto">
                                    <img src="img/int_rate_plot.png" class='img-fluid'></img>
                                    <h3 class="mb-3">Interest Rate by Defaults</h3>
                                    <p class="text-muted mb-0">This plot shows the distributions of interest rate for clients that defaulted and clients that fully paid back their loans.</p>
                                </div>
                            </div>
                            <div class="col-lg-3 text-center" style="margin: auto">
                                <div class="service-box mt-5 mx-auto">
                                    <img src="img/FICO_scores.png" class='img-fluid'></img>
                                    <h3 class="mb-3">FICO credit scores at loan inception</h3>
                                    <p class="text-muted mb-0">Here we see the FICO credit scores at the time the loan was granted to the customer. There is a hard cut at a FICO score of 660.</p>
                                </div>
                            </div>
                            <div class="col-lg-3 text-center" style="margin: auto">
                                <div class="service-box mt-5 mx-auto">
                                    <img src="img/last_FICO_scores.png" class='img-fluid'></img>
                                    <h3 class="mb-3">Current FICO credit scores</h3>
                                    <p class="text-muted mb-0">Here we see the FICO scores in real time. We can clearly see some borrowers who will eventually default have very poor scores.</p>
                                </div>
                            </div>
                        </div>

                        <style>

#sequence {
    width: 650px;
    height: 70px;
}

          #sequence text {
              font-weight: 600;
              fill: #fff;
          }

          #chart2 {
              position: relative;
          }

          #chart2 path {
              stroke: #fff;
          }

          #explanation {
              position: relative;
              text-align: center;
              color: #666;
              z-index: -1;
          }

          #percentage {
              font-size: 2.5em;
          }
                        </style>
                        </br>
                        <div class="container-fluid">
                            <!-- <div id="main"> -->
                            <div id="sequence"></div>
                            <div id="chart2">
                                <div id="explanation" style="visibility: hidden;">
                                </div>
                            </div>
                            <!-- </div> -->
                            <script src="js/helper.js" stype='text/javascript'></script>
                            <script> sundisk_plot("data/hir_sq2.json")

                            </script>
                            </br>
                            <h3>Dataset Composition</h3>
                            <p class="text-muted">We can see what percentage of the dataset different combinations of classes make up.
                            Here we show Loan Grade, Length of applicant employment, and Loan Term, respectively to get a feel for the composition of the dataset.</p>
                        </div>

                        </br>
                        <div class="container-fluid">
                            <style>
.axis path,
.axis line {
    fill: none;
    stroke: #000;
    shape-rendering: crispEdges;
}

              .x.axis path {
                  display: none;
              }

              .line {
                  fill: none;
                  stroke: steelblue;
                  stroke-width: 1.5px;
              }
                            </style>
                            <div id="chart3"></div>
                            <script src="js/helper.js" stype='text/javascript'></script>
                            <script> plot_line("data/amnt_data.csv")
                            </script>
                            </br>
                            <h3>Loan Amount over Time</h3>
                            <p class="text-muted">In this plot we can see the average loan amount change over time.
                            The loan amounts are split into two groups based on whether they eventually
                            defaulted or fully paid back the loan.  Here we can see a general upward trend in loan amount
                            and a general trend that loans defaulted on are higher than loans fully paid back. </p>
                        </div>
                        <hr class="my-4">
                    </div>
                </div>
            </div>
        </section>





<!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<!-- +++++++++++++++++++++++++ LIT REVIEW SECTION +++++++++++++++++++++++++++++ -->
<!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->

                    <section id="literature-review" style="background-color:#ecffea">
                        <div class="container" id="lit">

                            <div class="row">
                                <div class="col-lg-12 text-center">
                                  <div class="col-lg-12 text-center">
                                      <h2 class="section-heading">Literature Review</h2>
                                      <hr class="my-4">
                                  </div>

                                    <p>Here we review the papers that were most useful to us in deciding which direction
                                    to follow throughout the project.</p>

                                    <div class="list-group text-left">
                                        <a href="https://arxiv.org/pdf/1602.04938.pdf" class="list-group-item list-group-item-action flex-column align-items-start left-border-list">
                                            <div class="d-flex w-100 justify-content-between">
                                                <h5 class="mb-2">Local Interpretable Model-agnostic Explanations: LIME</h5>
                                            </div>
                                            <p class="mb-1">
                                            LIME is one of the most important elements of our final dashboard.  we use this method to
                                            explain the local results of the model in question and how much they influence the final decision
                                            of the model. LIME fits a local linear approximation with a subset of the available features
                                            to the function at a single point and uses the value of the coefficient on the features to determine which
                                            ones are the most important.
                                            </p>
                                            <small>Ribeiro, M.T., Singh, S. and Guestrin, C., 2016, August. Why should i trust you?: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1135-1144). ACM.</small>
                                        </a>
                                        <a href="https://arxiv.org/abs/1611.07634" class="list-group-item list-group-item-action flex-column align-items-start left-border-list">
                                            <div class="d-flex w-100 justify-content-between">
                                                <h5 class="mb-2">Interpretation of Prediction Models Using the Input Gradient</h5>
                                            </div>
                                            <p class="mb-1 text-left">
                                            Input Gradient is another critically important method we use in our dashboard's
                                            interpretability sections.  Input Gradients work by taking the gradient with respect to the x values
                                            (inputs) of the model to see how the result of the model changes for small changes in the input.
                                            Presumably, the features with a large gradient (i.e. that cause large change in the result of
                                            the model for a small change to the feature's value), are the features that effect the output of the
                                            model the most.
                                            </p>

                                            <small>Hechtlinger, Y., 2016. Interpretation of Prediction Models Using the Input Gradient. arXiv preprint arXiv:1611.07634.</small>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1701.08230.pdf" class="list-group-item list-group-item-action flex-column align-items-start left-border-list">
                                            <div class="d-flex w-100 justify-content-between">
                                                <h5 class="mb-2">Algorithmic decision making and the cost of fairness</h5>
                                            </div>
                                            <p class="mb-1 text-left">
                                            This paper was one of our favorites and gave us the four main metrics we use to
                                            evaluate the fairness of a model.  This paper goes in depth with different statistical
                                            definitions of fairness and the justification for each metric.  The four metrics we use from
                                            this paper are statistical parity, conditional statistical parity, false positive rate, and
                                            false negative rate.
                                            </p>
                                            <small>Corbett-Davies, S., Pierson, E., Feller, A., Goel, S. and Huq, A., 2017, August. Algorithmic decision making and the cost of fairness. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 797-806). ACM.</small>
                                        </a>

                                    </div>
                                </div>

                            </div>


                    <div class="row mt-5">
                        <div class="col-lg-12 text-left">
                            <h4>References:</h4>
                            <div class="list-group mt-3">
                                <a href="https://arxiv.org/abs/1606.03490" class="list-group-item list-group-item-action">Lipton, Z.C., 2016. <b>The mythos of model interpretability.</b> arXiv preprint arXiv:1606.03490.</a>
                                <a href="https://arxiv.org/pdf/1706.08519.pdf" class="list-group-item list-group-item-action">
                                    Corbett-Davies, S., Pierson, E., Feller, A., Goel, S. and Huq, A., 2017, August. <b>Algorithmic decision making and the cost of fairness</b>. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 797-806). ACM.
                                </a>
                                <a href="https://arxiv.org/abs/1610.08452" class="list-group-item list-group-item-action">
                                    Zafar, M.B., Valera, I., Gomez Rodriguez, M. and Gummadi, K.P., 2017, April. <b>Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment.</b> In Proceedings of the 26th International Conference on World Wide Web (pp. 1171-1180). International World Wide Web Conferences Steering Committee.

                                </a>
                                <a href="https://arxiv.org/abs/1601.05764" class="list-group-item list-group-item-action">
                                    Fish, B., Kun, J. and Lelkes, Á.D., 2016, June. <b>A confidence-based approach for balancing fairness and accuracy.</b> In Proceedings of the 2016 SIAM International Conference on Data Mining (pp. 144-152). Society for Industrial and Applied Mathematics.
                                </a>
                                <a href="https://arxiv.org/abs/1412.3756" class="list-group-item list-group-item-action">
                                    Feldman, M., Friedler, S.A., Moeller, J., Scheidegger, C. and Venkatasubramanian, S., 2015, August. <b>Certifying and removing disparate impact.</b> In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 259-268). ACM.
                                </a>
                                <a href="http://proceedings.mlr.press/v81/dwork18a/dwork18a.pdf" class="list-group-item list-group-item-action">
                                    Dwork, C., Immorlica, N., Kalai, A.T. and Leiserson, M.D., 2018, January. <b>Decoupled Classifiers for Group-Fair and Efficient Machine Learning.</b> In Conference on Fairness, Accountability and Transparency (pp. 119-133).</a>
                                <a href="https://arxiv.org/pdf/1703.04730.pdf" class="list-group-item list-group-item-action">
                                    Koh, P.W. and Liang, P., 2017. <b>Understanding black-box predictions via influence functions.</b> arXiv preprint arXiv:1703.04730.
                                </a>
                                <a href="https://arxiv.org/pdf/1802.00682.pdf" class="list-group-item list-group-item-action">Narayanan, M., Chen, E., He, J., Kim, B., Gershman, S. and Doshi-Velez, F., 2018. <b>How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation.</b> arXiv preprint arXiv:1802.00682.</a>
                                <a href="https://arxiv.org/pdf/1609.07236.pdf" class="list-group-item list-group-item-action">Friedler, S.A., Scheidegger, C. and Venkatasubramanian, S., 2016. <b>On the (im) possibility of fairness</b>. arXiv preprint arXiv:1609.07236.</a>
                            </div>
                        </div> </div>

                        <hr class="my-4">
                        </div>
                        </div>
                        </div>

                </section>


  <!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
  <!-- +++++++++++++++++++++++++ APPROACH SECTION +++++++++++++++++++++++++++++ -->
  <!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->



        <section id="approach">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Approach</h2>
                    <hr class="my-4">
                </div>
              <div class="container" id="models">
                <p style>
                Our main objective during this project was to generate a responsive report that could give insights on fairness and interpretability measures
                in a model-agnostic way. To accomplish this, we generated different models, implemented fairness methods and interpretability techniques. We describe the work below.
                </p>
                <h3 class="section-heading mt-5">Models</h3>
                <hr style="max-width: 100%; ">
                <p>
                Our project revolves around determining accurate metrics for fairness and interpretability, so we naturally needed a
                varied set of models to test our algorithms on. With that in mind, we decided to train 7 different models on the LendingClub
                dataset, with varying parameters and values, so that we could later on test our interpretability metrics on them. It is important to remark
                that accuracy was not our prime objective, but we were instead looking for varied models to refine our final deliverable: the dynamic report.
                The main models analyzed were:
                </p>

                <ul>Logistic Regression</ul>
                <ul>Decision Tree</ul>
                <ul>Random Forest</ul>
                <ul>AdaBoost</ul>
                <ul>XGBoost</ul>
                <ul>LightGBM</ul>
                <ul>Multi-Layer Perceptron</ul>

            </div>
            <!-- <br> -->
            <div class="container">
                <p>
                The training problem was set up as follows: we tried to predict if a particular client was going to default or repay their loan,
                given 114 features that describe their situation and characteristics (data points obtained from the publicly available LendingClub data).
                The data has a lot of categorical features and unusable predictors, so a round of data cleaning was needed.
                We removed useless predictors such as client id and loan id, transformed dates into categoricals and categoricals into one-hot encoded variables,
                removed columns with zero variance, and finally applied mean imputation to the cleaned dataset to fill out all the NaN values.
                </p>
                <p>
                We ended up with a dataset containing <span style="color:#5eb55b;">194 features</span>. This was fed to our models and we observed an unreasonably high accuracy on most of them.
                After this, we realized that some features clearly contained information posterior to the loan outcome. We removed most during the initial cleaning,
                but some remained. Features such as <span style="color:#5eb55b;">debt_settlement_flag</span>, a flag that indicates if a charged-off borrower is working with a debt settlement company,
                were easy to detect as post-facto, but other features such as <span style="color:#5eb55b;">funded_amnt</span>, the total amount committed by the loan,
                were harder to catch: it turned out to be the total amount after a repay
                </p>
                <p>
                After removing the post-facto features, we ended up with a new model with <span style="color:#5eb55b;">150 features</span> that had much more realistic results. We operated
                hyperparameter search with a grid search approach on all of our models. We show below the parameters tried and the best parameters found.
              </p>

                <div class="container_expand_on_hover" id="container_expand_on_hover">

                    <div class ="item_exp" id="logreg">
                        <div class="content">
                            <h4>Logistic Regression</h4>
                            <p>
                            - Accuracy: 79.8% <br>
                            - AUC: 0.68 <br>
                            - Max-depth of tree: None
                            </p>
                        </div>
                    </div>

                    <div class ="item_exp" id="decision_tree">
                      <div class="content">
                        <h4>Decision Tree</h4>
                        <p>
                        - Accuracy: 79.8% <br>
                        - AUC: 0.68 <br>
                        - Max-depth of tree: None
                        </p>
                      </div>
                    </div>

                  <div class ="item_exp" id="adaboost">
                    <div class="content">
                      <h4>AdaBoost</h4>
                      <p>
                      - Accuracy: 79.9% <br>
                      - AUC: 0.72 <br>
                      - Number of estimators: 100
                      </p>
                    </div>
                  </div>

                  <div class ="item_exp" id="xgb">
                    <div class="content">
                      <h4>XGBoost</h4>
                      <p>
                      - Accuracy: 80.3% <br>
                      - AUC: 0.73 <br>
                      - Max-depth: 8. Number of estimators: 100
                      </p>
                    </div>
                  </div>

                  <div class ="item_exp" id="LGBM">
                    <div class="content">
                      <h4>LGBM</h4>
                      <p>
                      - Accuracy: 80.4% <br>
                      - AUC: 0.73 <br>
                      - Number of leaves: 31
                      </p>
                      <!-- <p>Number of leaves. Tried 31, 41, 51.</p> -->
                    </div>
                  </div>

                  <div class ="item_exp" id="mlp">
                    <div class="content">
                      <h4>Multi-Layer Perceptron</h4>
                      <p>
                      - Accuracy: 80.4% <br>
                      - AUC: 0.73 <br>
                      - Hidden layer sizes: 50x50x50
                      </p>
                      <!-- <p>Hidden layer sizes. Tried 1x100, 2x100 and 3x50.</p> -->
                    </div>
                  </div>

                  <div class ="item_exp" id="rf">
                    <div class="content">
                      <h4>Random Forest</h4>
                      <p>
                      - Accuracy: 79.8% <br>
                      - AUC: 0.68 <br>
                      - Number of estimators: 100. Min samples per leaf: 5
                      </p>
                      <!-- <p>Number of estimators (trees) and min samples per leaf. Tried 10, 50, 100, 200 and 1,5,10,25.</p> -->
                    </div>
                  </div>

                </div>

            <script  type="text/javascript">
              $(function() {
                $( "#model_table_btn" ).click(function() {
                    $( "#model_table" ).toggle('slow', function() {
                      // Animation complete.
                    });
                    });
                  });

              function ShowModelImage() {
                  document.getElementById("model_table").style.display = "block";
                  document.getElementById("model_table_btn").innerHTML = "Hide static image"
              }
            </script>

          <div class="container" >
            <div class="col-lg-8 mx-auto text-center">
                <a class="btn btn-light mb-5" id="model_table_btn" style="margin:auto;">Show image</a>
                <br>
                <img src="img/model_table.png" class="img-fluid mb-5" id="model_table">
            </div>
          </div>
          <p>
            As this is a highly imbalanced problem, we also obtained ROC curves and confusion matrices for our models, to further present the
            performance of our model. These figures are part of the final report that we generate later on. We present some of them below.
          </p>

          </div>

                    </div>

            <div class='container'>

                <div class="row" style="display:flex; justify-content: center;">
                    <img class="img-fluid" src="img/ROC_LGBM.png" style="max-height:250px;width:auto">
                    <img class="img-fluid" src="img/cm_LGBM.png" style="max-height:250px;width:auto">
                </div>
                <!-- <div class="row" style="display:flex; justify-content: center;">
                    <img class="img-fluid" src="img/ROC_XGB.png" style="max-height:250px;width:auto">
                    <img class="img-fluid" src="img/cm_XGB.png" style="max-height:250px;width:auto">

                </div> -->
            </div>




<!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<!-- +++++++++++++++++++++++++ INTERP SUBSECTION +++++++++++++++++++++++++++++ -->
<!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->

  <div class="container" id="interp">
    <h2 class="section-heading mt-5">Interpretability</h2>
    <hr style="max-width: 100%">
    <h4 class="section-heading mt-5">Driving understanding for the credit analyst</h4>
    <p>Local interpretation models are the core of our interpretation
      functionality.
      Our objective for local interpretations is to take a
      black-box model and its training set and return a ranked set
      of features that
      drive the model outcome.</p>

    <h4 class="section-heading mt-5">Back-end: Reasons For Decision</h4>
    <p>We worked with three primary methodologies to produce local explanations for a given observation:</p>
    <ul><span>LIME</span>: First proposed by Ribeiro, Singh, and Guestrin (2016), LIME is an explanation technique that creates an ad hoc
      linear model in the local region of the decision manifold.  This is done by applying small input perturbations near the observation.
      For EXIGO, we use a third-party LIME SDK and add several functionalities (see next section).</ul>
    <ul><span>Input Gradients</span>: Initially proposed by Hechtlinger (2016), this method approximates the distance to the decision
      boundary by taking the gradient of the output with respect to the feature set. We implement this technique in a python module.</ul>
    <ul><span>Influence Functions</span>: We attempted to utilize the Influence Functions method proposed by Koh and Lang,
      a loss-based method that calculates how different features affect the gradient of the in-sample loss.</ul>
    <p>
      We ultimately discarded this methodology, though, as it requires a well-defined loss
      function and therefore breaks our requirement of working with black-box models.
      We attempted to circumvent this issue by reproducing a black-box model with a feed-forward neural network -
      this would in theory generate a loss function for a black-box model and hence allow us to utilize Influence Functions.
      However, in practice this was unproductive; the neural net was an "approximation of an approximation"
      and we had difficulty reproducing an arbitrary model with any practical accuracy.
    </p>

    <h4 class="section-heading mt-5">Back-end: Confidence In Calculated Decision Reasons</h4>
    <p>Each of our selected feature estimators (LIME and Input Gradients) require a linearization on the
      decision manifold in the local region around the observation.  As such, there is inherent error in the explanations.
      While we cannot eliminate the error, the next best thing is to provide a confidence in the output.  We built three methods for evaluating the confidence in the reasons for any given observation:</p>
		<ul><span>LIME local perturbation</span>: LIME builds a local linear approximation to the model's decision boundary at the point of interest based on random perturbations to the input vector.  We capture the accuracy of LIME's approximation by running multiple instances of the random perturbation and quantifying how the LIME output changes.</ul>
    <ul><span>LIME stochasticity testing</span>: Further, we perform our own perturbations to the input into LIME and measure the divergence of explanations in the local region.  We believe this will capture the linearity error plus errors inherent in the LIME method.</ul>
    <ul><span>Hessian</span>: The Hessian of the decision manifold is a measure of curvature.  As a result, it is naturally suited to providing a measure of local non-linearity, which in turn informs the error of the local approximation.</ul>

    <h4 class="section-heading mt-5">Front-end: Local Plots used in Exigo</h4>
    <p>We used the various local explanation methods and their respective confidence measures to produce a number of visualizations for the end-user.  These are:</p>
    <ul><span>LIME plot</span>: The LIME plot is the standard output used by line, showing feature importances,
      with our error bars from the LIME stochasticity testing overlaid above.
      Its purpose is to provide the user with most important reasons for each individual decision.</ul>
    <ul><span>LICE plot</span>: The LICE plot is an extension of the ICE plot, which computes partial
      dependency quantile plots for given features and overlays a what-if analysis for the selected observation.
      We run this for the 5 most important features in the dataset (as selected by LIME).
      The purpose of this plot is twofold: first, it visualizes the overall shape of the data with respect to each feature,
      and secondly, it highlights the observation's relative position in the dataset.</ul>
    <ul><span>Confidence score (local accuracy)</span>: This is simply a one-number visualization of how many of the LIME-perturbed points are accurate to the base model under LIME's linear approximation model.</ul>

    <h4 class="section-heading mt-5">Local/Global Patterns</h4>
    <p>Having access to observation-level decision reasons opens up a world of analysis possibilities at the aggregate level.  We are able to detect regional or global patterns in these reasons and visualize them to provide unique glimpses into the model's performance and the shape of its input data.  The visualizations we performed are:</p>
    <ul><span>GRIME</span>: GRIME is an aggregation of most important features, achieved by taking the mean of each local feature importance
      across a significant subsample of observations.  We note that certain models already offer techniques for
      global-level feature importance (RandomForest, XGBoost, etc),
      however our model-agnostic solution allows this to be performed on any binary classifier.
      We quantify error in this method with the standard deviations of the individual feature importances for each observation.</ul>
    <ul><span>DICE</span>: The DICE plot is created by dividing the dataset into default-probability quantiles
      and using local importance to find the most common positive- and negatively- correlated features
      for each quantile.  These are overlaid onto a PDF of the dataset, resulting in a visualization
      that correlates feature importance with both loan risk as well as overall frequency of observation.</ul>
    <ul><span>BILE</span>: BILE is our unique method of plotting decision boundaries for important features.
      First, KD-trees were selected as a fast ( O(log(n)) ) method for searching
      nearest-neighboring observations.  We compute these nearest-neighbors when
      compressing the highly-dimensional decision boundary into two-dimensional space,
      and then apply a contour plot to visualize default probabilities.
      The resulting plot shows the interrelation between two important features and
      the response variable. We recognize that our KD-Tree based approach represents
      a maximum likelihood approach to sampling other non-displayed features in the
      dataset.  If we approached this from a bayesian perspective, we could get nice
      error bounds, but we find the bayesian approach computationally burdensome, so
      we stick with the maximum likelihood approach.</ul>

    <h6 class="section-heading m-3">Discarded attempts:</h6>
    <ul><span>PCA/K-Means clustering</span>: We attempted to reduce the dimensionality
      using PCA and clustering over the principal components to deduce interesting global-level
      insights.  In practice, we found that the first few principal components often tended
      to be dominated by a single feature.  Although this is useful proxy for feature importance,
      we already have strong tools for this so we discarded this redundant functionality.</ul>
    <ul><span>T-SNE</span>: We also attempted to visualize the space with T-SNE.
      First we used an initial dimensionality reduction technique to reduce to less than 50 dimensions
      (as T-SNE often underperforms on manifolds over 50 dimensions).  We attempted to
      label clusters using various feature and response combinations.  However, the resulting
      clusters were occasionally insightful and T-SNE was inherently too unpredictable as a methodology, so we chose to discard it.</ul>
  </div>


<!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<!-- +++++++++++++++++++++++++ FAIRNESS SUBSECTION +++++++++++++++++++++++++++++ -->
<!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->

                <div class="container text-justified">
                <h2 class="section-heading mt-5">Fairness</h2>
                <hr style="max-width: 100%; ">

                <h4 class="section-heading mt-5">Why Fairness?</h4>
                <p>
                As Artificial Intelligence becomes more powerful, algorithm-based decisions have become increasingly common for various problems such as loan lending or detention. With the trend comes the issue of ensuring fairness along with other key aspects such as accountability or transparency. According to General Data Protection Regulation (GDPR) of the European Union, for instance, anyone, subject to an algorithmic decision, has a right to ask for an explanation. As such policies go into effect, explaining a black-box model such as neural network and controlling undesirable patterns in its decision-making become the key to its successful application. When the legal framework is set in place, organizations that use algorithms to make importance decisions could face a legal risk. If their model were to exhibit statistically significant discrimination, they may face a class action lawsuit. As a first step, we study different ways of quantifying the fairness.
                </p>

                <h4 class="section-heading mt-5">Is My Black-box Model Fair?</h4>
                <p>
                We present the three, perhaps most widely used, metrics that quantify the level of fairness. These are global fairness metrics because they look at how the model in question makes decisions <i>overall</i> for a protected group vs. the entire population. By protected group, we mean groups based on sensitive characteristics such as gender, religion, or race. When an algorithm is completely rule-based where each rule is specified manually by human, it is relatively easy to evaluate its fairness because one can check the rules one by one. However, the powerful inference models we use today have complex inner-workings to inspect and hence, usually the best we can do is understand their statistical behaviors. Most of the best inference models we use today take an probabilistic approach and hence, we measure the fairness in terms of expectations.
                </p>

                <div class="row">
                    <div class="col-lg-3 text-center" style="margin: auto">
                        <div class="service-box mt-5 mx-auto">
                            <h4 class="mb-3">Statistical Parity</h4>
                            <p class="text-muted mb-0">shows an equal portion of protected class and general population is expected to receive loans. The bias can be defined as the absolute difference of the two terms below.</p>
                            <img src="img/statistical_parity.png" class='img-fluid mt-3' style="height:20px"/>
                        </div>
                    </div>

                    <div class="col-lg-3 text-center" style="margin: auto">
                        <div class="service-box mt-5 mx-auto">
                            <h4 class="mb-3">Conditional Parity</h4>
                            <p class="text-muted mb-0">shows an equal portion of protected class and general population is expected to receive loans, conditioning on a set of pre-specified features. The bias can be defined in the same way.</p>
                            <img src="img/conditional_parity.png" class='img-fluid mt-3' style="height:20px"/>
                            <!--img src="img/conditional_parity.png" class='img-fluid mt-3' style="height:22px"></img-->
                        </div>
                    </div>

                    <div class="col-lg-3 text-center" style="margin: auto">
                        <div class="service-box mt-5 mx-auto">
                            <h4 class="mb-3">Predictive Equality</h4>
                            <p class="text-muted mb-0">shows an equal level of accuracy is expected for protected class and general population as measured by False Positive Rate (FPR) and False Negative Rate (FNR).</p>
                            <img src="img/fpr.png" class='img-fluid mt-3 mr-3' style="display:inline-block;height:40px"></img>
                            <img src="img/fnr.png" class='img-fluid mt-3' style="display:inline-block;height:40px"></img>
                        </div>
                    </div>
                    <!--div class="col-lg-3 text-center" style="margin: auto">
                        <div class="service-box mt-5 mx-auto">
                        <h4 class="mb-3">False Positive Rate</h4>
                        <p class="text-muted mb-0">shows an equal portion of protected class and general population is expected to receive loans.</p>
                        <img src="img/fpr.png" class='img-fluid mt-3' style="height:22px"></img>
                        </div>
                        </div>

                        <div class="col-lg-3 text-center" style="margin: auto">
                        <div class="service-box mt-5 mx-auto">
                        <h4 class="mb-3">False Negative Rate</h4>
                        <p class="text-muted mb-0">shows an equal portion of protected class and general population is expected to receive loans.</p>
                        <img src="img/fnr.png" class='img-fluid mt-3 pd-5' style="height:22px"></img>
                        </div>
                        </div-->
            </div>

            <h4 class="section-heading mt-5">Implementation & Use Cases</h4>
            <!-- <div class="text-center mt-5 mb-5">
                <img src="img/fairness_demo.png" class="img-fluid" />
                <caption>The screenshot of the fairness section in the report.</caption>
            </div> -->
            <p>
            We implemented an app that generates, on the fly, the report that shows the three basic evaluation methods, given your model and data. Since all these evaluation metrics are statistical tests we have a probability value with which we must assign a "pass" or "fail" value.  We choose a 95% confidence level based on generally accepted statistical practices in industry. The evaluations also all come with confidence intervals to show variability of the fairness measures. Why is this useful? Suppose you are a CEO of a financial company that lends money to people. Under regulations like Equal Credit Opportunity Act (ECOA), you are obliged to comply with the regulation and should you violate, your business may face serious legal and financial issues. In addition, suppose you are a credit analyst. This section of the report may seem unimportant to a credit analyst making a decision for a single loan, but it is still important to the report because it allows the credit analyst to verify the model she is being assisted by treats people fairly on the basis of their membership in various protected classes. This report not only verifies that the assisting model is fair, but it shows that the assisting model is compliant with the applicable laws of the land.
            </p>

            <h4 class="section-heading mt-5">Future Work</h4>
            <p>
            There are many open research questions. So far, we assume we are given a fully-trained black-box model and that we cannot change the model. One important question is how can we bake in the notion of fairness to the training of a black-box model from the beginning? Another interesting direction is to measure the level of fairness in the decisions made for an individual, regardless of the global statistical behavior of the model (local fairness).
            </p>
          </br>

        </div>



<!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<!-- +++++++++++++++++++++++++ TESTING SUBSECTION +++++++++++++++++++++++++++++ -->
<!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->


            <div class="container">
            <h2 class="section-heading mt-5">Testing</h2>
            <hr style="max-width: 100%">
            <p>
            Testing is an essential part of ensuring our dashboard actually performs the task we are building it to perform.
            Since our goal for the report is to aid in the loan approval decision a credit analyst must make, it makes sense to
            pursue human evaluation testing. Although human evaluation testing is the direction we would like to pursue to
            evaluate our report, we sadly do not have access to the number of credit analysts necessary to perform a legitimate
            human evaluation test. </p>
                </br>
                <div class="container-fluid">
                    <div class="row">
                        <div class="col-lg-4  col-md-6  text-center">
                            <div class="service-box mt-5 mx-auto">
                                <i class="fa fa-4x  fa-cogs mb-3 sr-icons" data-sr-id="5" style="; visibility: visible;  -webkit-transform: scale(1); opacity: 1;transform: scale(1); opacity: 1;-webkit-transition: -webkit-transform 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s, opacity 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s; transition: transform 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s, opacity 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s; "></i>

                                <p class="text-muted mb-0">We propose to test our dashboard with a speed comparison between analysts with and without our dashboard. </p>
                            </div>
                        </div>
                        <div class="col-lg-4 col-md-6 text-center">
                            <div class="service-box mt-5 mx-auto">
                                <i class="fa fa-4x  fa-user mb-3 sr-icons" data-sr-id="5" style="; visibility: visible;  -webkit-transform: scale(1); opacity: 1;transform: scale(1); opacity: 1;-webkit-transition: -webkit-transform 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s, opacity 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s; transition: transform 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s, opacity 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s; "></i>

                                <p class="text-muted mb-0">The testing process will also control for accuracy to make sure we provide accurate, not just decisive, insights. </p>
                            </div>
                        </div>
                        <div class="col-lg-4 col-md-6 text-center">
                            <div class="service-box mt-5 mx-auto">
                                <i class="fa fa-4x  fa-check-square mb-3 sr-icons" data-sr-id="5" style="; visibility: visible;  -webkit-transform: scale(1); opacity: 1;transform: scale(1); opacity: 1;-webkit-transition: -webkit-transform 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s, opacity 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s; transition: transform 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s, opacity 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s; "></i>

                                <p class="text-muted mb-0">A human evaluation test like this will be able to validate if our dashboard adds tangible value to a credit analyst.</p>
                            </div>
                        </div>
                    </div>
                </div>

            </br>
            <p> Although this testing setup would not be very difficult to execute given the right resources, plenty of credit analysts, we do
            not have access to the necessary resources and were not be able to perform this testing. </p>

          </div>


          </section>




  <!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
  <!-- +++++++++++++++++++++++++ Results SECTION +++++++++++++++++++++++++++++ -->
  <!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->


        <section id="Results" class=bg-primary>
            <div class="container">
                <div class="row">
                    <div class="col-lg-12 text-center">

                        <h2 class="section-heading text-white">Results</h2>
                        <hr class="light my-4">
                        <p class="text-faded mb-4">Our final result is a dashboard with four pages that hits the main themes we touched in the above section. The first
                        page covers the model's performance with the metrics like AUC and Accuracy.  The second page covers the global fairness of the model.
                      The final two pages of the dashboard are both about interpretability.  The first of these pages covers local interpretability, while the
                    last page covers global interpretability.</p><p class="text-white mb-4"> We could continue to describe our dashboard with text, but, as they say, a dashboard is worth 1000 words.</p>
                        <!-- <hr class="my-4"> -->
                    </div>
                </div>
            </div>
        </section>



  <!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
  <!-- +++++++++++++++++++++++++ ICONS SECTION +++++++++++++++++++++++++++++ -->
  <!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->



    <section id="demo">
                <div class="container">
                    <div class="row">
                        <div class="col-lg-12 text-center">
                            <h2 class="section-heading">The Dashboard</h2>
                            <hr class="my-4">
                        </div>
                    </div>
                </div>
                <div class="container">
                    <div class="row">
                        <div class="col-lg-3 col-md-6 text-center">
                            <div class="service-box mt-5 mx-auto">
                                <i class="fa fa-4x fa-user text-primary mb-3 sr-icons" data-sr-id="5" style="; visibility: visible;  -webkit-transform: scale(1); opacity: 1;transform: scale(1); opacity: 1;-webkit-transition: -webkit-transform 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s, opacity 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s; transition: transform 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s, opacity 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s; "></i>
                                <h3 class="mb-3">Interpretability</h3>
                                <p class="text-muted mb-0">Our methods deliver a fully feature aware interpretation for both global and local patterns.</p>
                            </div>
                        </div>
                        <div class="col-lg-3 col-md-6 text-center">
                            <div class="service-box mt-5 mx-auto">
                                <i class="fa fa-4x fa-balance-scale text-primary mb-3 sr-icons" data-sr-id="6" style="; visibility: visible;  -webkit-transform: scale(1); opacity: 1;transform: scale(1); opacity: 1;-webkit-transition: -webkit-transform 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s, opacity 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s; transition: transform 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s, opacity 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s; "></i>
                                <h3 class="mb-3">Fairness</h3>
                                <p class="text-muted mb-0">We include fairness metrics that give insights on how balanced and fair your model is, given a particular protected class.</p>
                            </div>
                        </div>
                        <div class="col-lg-3 col-md-6 text-center">
                            <div class="service-box mt-5 mx-auto">
                                <i class="fa fa-4x fa-cog text-primary mb-3 sr-icons" data-sr-id="7" style="; visibility: visible;  -webkit-transform: scale(1); opacity: 1;transform: scale(1); opacity: 1;-webkit-transition: -webkit-transform 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s, opacity 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s; transition: transform 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s, opacity 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s; "></i>
                                <h3 class="mb-3">Performance</h3>
                                <p class="text-muted mb-0">The report shows Accuracy, AUC, and will soon extend to other metrics. We provide
                                a straightforward and complete explanation of the model.</p>
                            </div>
                        </div>
                        <div class="col-lg-3 col-md-6 text-center">
                            <div class="service-box mt-5 mx-auto">
                                <i class="fa fa-4x fa-bug text-primary mb-3 sr-icons" data-sr-id="8" style="; visibility: visible;  -webkit-transform: scale(1); opacity: 1;transform: scale(1); opacity: 1;-webkit-transition: -webkit-transform 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s, opacity 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s; transition: transform 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s, opacity 0.6s cubic-bezier(0.6, 0.2, 0.1, 1) 0s; "></i>
                                <h3 class="mb-3">Debugging</h3>
                                <p class="text-muted mb-0">Knowing what features influenced each model's decision is key to finding unexpected bugs.</p>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="col-lg-8 mx-auto text-center">
                  <h2 class="section-heading mt-5">Why don't we just show you?</h2>
                    <p class="mt-3 text-muted">
                      The Demo currently runs on a LGBM Model trained with 10% of the data, so as to reduce processing delays during the presentation.
                      You will, nevertheless, be able to witness Exigo's full dynamic report generation on the fly.
                    </p>
                </div>
                <br>
                <div class="col-lg-8 mx-auto text-center" >
                    <a class="btn btn-primary btn-xl " href="http://wholesome-auditor.herokuapp.com">View Demo</a>
                </div>


            </section>




<!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<!-- +++++++++++++++++++++++++ DEMO SECTION +++++++++++++++++++++++++++++ -->
<!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->

        <!-- <section class="bg-primary" id="drag-and-drop">

            <div class="col-lg-8 mx-auto text-center">
                <p class="text-faded mb-4">
                  The Demo currently runs on a LGBMModel trained with 10% of the data, so as to reduce processing delays during the presentation.
                  You will be able to witness Exigo's dynamic report generation on the fly.
                </p>
            </div> -->

            <!-- <style>
            #control-container {
                padding: 0px !important;
            }

            .e-upload {
                width: 72%;
            }
            </style> -->

            <!-- <div class="container2"  role="main">

                <div class="box __input">
                <svg class="box__icon" xmlns="http://www.w3.org/2000/svg" width="50" height="43" viewBox="0 0 50 43">
                <path d="M48.4 26.5c-.9 0-1.7.7-1.7 1.7v11.6h-43.3v-11.6c0-.9-.7-1.7-1.7-1.7s-1.7.7-1.7 1.7v13.2c0 .9.7 1.7 1.7 1.7h46.7c.9 0 1.7-.7 1.7-1.7v-13.2c0-1-.7-1.7-1.7-1.7zm-24.5 6.1c.3.3.8.5 1.2.5.4 0 .9-.2 1.2-.5l10-11.6c.7-.7.7-1.7 0-2.4s-1.7-.7-2.4 0l-7.1 8.3v-25.3c0-.9-.7-1.7-1.7-1.7s-1.7.7-1.7 1.7v25.3l-7.1-8.3c-.7-.7-1.7-.7-2.4 0s-.7 1.7 0 2.4l10 11.6z"/></svg>
                <input type="file" name="files[]" id="file" class="box__file2" data-multiple-caption="{count} files selected" multiple />
                <label for="file"><strong>Choose a file</strong><span class="box__dragndrop"> or drag it here</span>.</label>
                <button type="submit" class="box__button">Upload</button>
                </div>

                <form>
                <div class="box__uploading">Uploading&hellip;</div>
                <div class="box__success">Done! <a href="https://css-tricks.com/examples/DragAndDropFileUploading//?" class="box__restart" role="button">Upload more?</a></div>
                <div class="box__error">Error! <span></span>. <a href="https://css-tricks.com/examples/DragAndDropFileUploading//?" class="box__restart" role="button">Try again!</a></div>
                </form>

                </div> -->
                <!-- <div class="col-lg-8 mx-auto text-center" style="padding-top:50px">
                    <a class="btn btn-light btn-xl " href="http://wholesome-auditor.herokuapp.com">View Demo</a>
                </div>
        </section> -->


        <section id="Conclusion" class="bg-dark">
            <div class="container" id="conclusion">
                <div class="row">
                    <div class="col-lg-12 text-center">
                        <h2 class="section-heading text-white">Conclusion</h2>
                        <hr class="my-4">

                        <p class="text-faded text-justified"> The primary use of our dashboard is for a credit analyst to make <span>quick and accurate
                        decisions</span>.  We approach this part of the problem by presenting both a high level intuition
                        for the model's decisions and a specific analysis of the model's decision for the specific instance
                        in question. The intuition part is accomplished by our <span>model performance</span> and <span>global patterns</span> sections.
                        The <span> local analysis</span> of the model's decision for the individual in question is central to the report and represents
                        the tool a credit analyst would primarily rely on. </p>

                        <p class="text-faded">However, just making a better informed decision on acceptable features is not enough to ensure
                        that the decision process is really fair to all people groups. We also include a <span>fairness
                        section</span> in our dashboard where we present <span>four standard statistical tests</span> for fairness. </p>

                        <p class="text-faded">We think this report will be helpful for credit analysts and enable them to make quick and
                        accurate loan approval decisions while having visibility to the assisting model's fairness. In addition,
                        we built the dashboard to be <span>model-agnostic</span> and suitable to report on any binary classification problem.</p>

                        <h2 class="section-heading text-white mt-5">Future Work</h2>
                        <hr class="my-4">

                        <p class="text-faded">There are many different directions we could take this project to improve it. A necessary next
                        step is to implement the <span>human evaluation testing</span> we propose in the
                        Approach section of the report. Some other interesting next steps include offering
                        a <span>"fairness slider"</span> where the user can change the fairness for their model and
                        see the impact on accuracy of the model. The final proposal for future work, and the
                        most interesting to us as a team, is a <span>generalization of the dashboard</span> to support both
                        multinomial classification and regression models.
                        </p>

                    </div>
                </div>
            </div>
        </section>


        <section id="contact">
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 mx-auto text-center">
                        <h2 class="section-heading">Contact the team!</h2>
                        <hr class="my-4">
                        <p class="mb-5">Any questions on Exigo? Don't hesitate to contact us! We will be happy to answer any inquiry and we will get back to you as soon as possible!</p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-lg-4 ml-auto text-center">
                        <i class="fa fa-phone fa-3x mb-3 sr-contact"></i>
                        <p>123-456-6789</p>
                    </div>
                    <div class="col-lg-4 mr-auto text-center">
                        <i class="fa fa-envelope-o fa-3x mb-3 sr-contact"></i>
                        <p>
                        <a href="mailto:your-email@your-domain.com">pavlos@protopapas.com</a>
                        </p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Bootstrap core JavaScript -->
        <script src="vendor/jquery/jquery.min.js"></script>
        <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

        <!-- Plugin JavaScript -->
        <script src="vendor/jquery-easing/jquery.easing.min.js"></script>
        <script src="vendor/scrollreveal/scrollreveal.min.js"></script>
        <script src="vendor/magnific-popup/jquery.magnific-popup.min.js"></script>

        <!-- Custom scripts for this template -->
        <script src="js/creative.min.js"></script>


        <!-- D3 for visulaization -->
        <!-- <script src="js/d3.v4.min.js" charset="utf-8"></script> -->

        <!-- Scripts for Waves -->
        <script src="waves_animation/waves_files/three.js.download"></script>
        <script src="waves_animation/waves_files/Projector.js.download"></script>
        <script src="waves_animation/waves_files/CanvasRenderer.js.download"></script>
        <script src="waves_animation/waves_files/stats.min.js.download"></script>

        <script>

            var SEPARATION = 140, AMOUNTX = 40, AMOUNTY = 40;

var container, stats;
var camera, scene, renderer;

var particles, particle, count = 0;

var mouseX = 0, mouseY = 0;

var windowHalfX = window.innerWidth / 2;
var windowHalfY = window.innerHeight / 2;

init();
animate();

function init() {
    // var container = createCanvas(window.innerWidth, window.innerHeight);
    // container.parent("sketch-holder");
    // background(0);

    container = document.getElementById( 'sketch-holder' );
    // document.body.appendChild( container );

    camera = new THREE.PerspectiveCamera( 75, window.innerWidth / window.innerHeight, 1, 100000 );
    camera.position.z = 1000;
    camera.position.y = 2000;

    scene = new THREE.Scene();

    particles = new Array();

    var PI2 = Math.PI * 2;
    var material = new THREE.SpriteCanvasMaterial( {

        color: 0x5eb55b,
        program: function ( context ) {

            context.beginPath();
            context.arc( 0, 0, 0.5, 0, PI2, true );
            context.fill();

        }

    } );

    var i = 0;

    for ( var ix = 0; ix < AMOUNTX; ix ++ ) {

        for ( var iy = 0; iy < AMOUNTY; iy ++ ) {

            particle = particles[ i ++ ] = new THREE.Sprite( material );
            particle.position.x = ix * SEPARATION - ( ( AMOUNTX * SEPARATION ) / 2 );
            particle.position.z = iy * SEPARATION - ( ( AMOUNTY * SEPARATION ) / 2 );
            scene.add( particle );

        }

    }

    renderer = new THREE.CanvasRenderer();
    renderer.setPixelRatio( window.devicePixelRatio );
    renderer.setSize( window.innerWidth, window.innerHeight );
    container.appendChild( renderer.domElement );

    // stats = new Stats();
    // container.appendChild( stats.dom );

    document.addEventListener( 'mousemove', onDocumentMouseMove, false );
    document.addEventListener( 'touchstart', onDocumentTouchStart, false );
    document.addEventListener( 'touchmove', onDocumentTouchMove, false );

    //

    window.addEventListener( 'resize', onWindowResize, false );

}

function onWindowResize() {

    windowHalfX = window.innerWidth / 2;
    windowHalfY = window.innerHeight / 2;

    camera.aspect = window.innerWidth / window.innerHeight;
    camera.updateProjectionMatrix();

    renderer.setSize( window.innerWidth, window.innerHeight );

}

//

function onDocumentMouseMove( event ) {

    mouseX = event.clientX - windowHalfX;
    mouseY = event.clientY - windowHalfY;

}

function onDocumentTouchStart( event ) {

    if ( event.touches.length === 1 ) {

        event.preventDefault();

        mouseX = event.touches[ 0 ].pageX - windowHalfX;
        mouseY = event.touches[ 0 ].pageY - windowHalfY;

    }

}

function onDocumentTouchMove( event ) {

    if ( event.touches.length === 1 ) {

        event.preventDefault();

        mouseX = event.touches[ 0 ].pageX - windowHalfX;
        mouseY = event.touches[ 0 ].pageY - windowHalfY;

    }

}

//

function animate() {

    requestAnimationFrame( animate );

    render();
    // stats.update();

}

function render() {

    camera.position.x += ( mouseX - camera.position.x ) * .05;
    camera.position.y += ( - mouseY - camera.position.y + 350) * .05;
    camera.lookAt( scene.position );

    var i = 0;

    for ( var ix = 0; ix < AMOUNTX; ix ++ ) {

        for ( var iy = 0; iy < AMOUNTY; iy ++ ) {

            particle = particles[ i++ ];
            particle.position.y = ( Math.sin( ( ix + count ) * 0.3 ) * 50 ) +
                ( Math.sin( ( iy + count ) * 0.5 ) * 50 );
            particle.scale.x = particle.scale.y = ( Math.sin( ( ix + count ) * 0.3 ) + 1 ) * 4 +
                ( Math.sin( ( iy + count ) * 0.5 ) + 1 ) * 4;

        }

    }

    renderer.render( scene, camera );

    count += 0.1;

}

        </script>

    </body>

</html>
